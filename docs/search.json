[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jenetty Immaraj",
    "section": "",
    "text": "Linkedin\n  \n  \n    \n     Resume\n  \n  \n    \n     Email\n  \n  \n    \n     Github\n  \n\n  \n  \nWelcome to my professional portfolio! With over 8 years of seasoned expertise in data science and a proven track record in architecting and deploying ML solutions on leading cloud platforms, I specialize in solving complex business challenges across various industries. Passionate about leveraging technical proficiency and creativity, I strive to drive impactful contributions that propel organizational growth and development.\n\n\nUniversity of North Texas, Denton | Dallas, TX. MS in Data Science | Aug 2022 - Dec 2023.\nCarnegie Mellon University| Forbes Ave, PA. PGP in Data Science | Jul 2016 - Apr 2017.\n\n\n\nGenpact | Sr Data Scientist | Mar 2021 - Aug 2022\nQuadratyx | Data Scientist | Jan 2019 - Feb 2021\nVirtusa | Software Engineer | Aug 2015 - Jan 2019"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jenetty Immaraj",
    "section": "",
    "text": "University of North Texas, Denton | Dallas, TX. MS in Data Science | Aug 2022 - Dec 2023.\nCarnegie Mellon University| Forbes Ave, PA. PGP in Data Science | Jul 2016 - Apr 2017."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Jenetty Immaraj",
    "section": "",
    "text": "Genpact | Sr Data Scientist | Mar 2021 - Aug 2022\nQuadratyx | Data Scientist | Jan 2019 - Feb 2021\nVirtusa | Software Engineer | Aug 2015 - Jan 2019"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "vjenettyimmaraj@gmail.com"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Analysis and predictive assessment of donation amounts.\n\n\n\n\n\n\n\nanalysis\n\n\nSAS Enterprise Miner\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nVeda Jenetty Immaraj, Eric Droegemeier, Gregory Ehlinger\n\n\n\n\n\n\n  \n\n\n\n\nLeBron James Career Breakdown\n\n\n\n\n\n\n\nAnalysis\n\n\nTableau\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nVeda Jenetty Immaraj, Gregory Ehlinger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/posts/post-with-code/index.html",
    "href": "docs/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Analysis and Prediction on Donations/index.html",
    "href": "posts/Analysis and Prediction on Donations/index.html",
    "title": "Analysis and predictive assessment of donation amounts.",
    "section": "",
    "text": "Examine the historical data pertaining to contributions made to a non-profit organization and assess the efficacy of predictive models in forecasting future donations.\nDonation Project Download\nDonation Report Download\nNon-profit Donation Dataset Download\n\nIn this project, Eric Droegemeier, Gregory Ehlinger, and I conducted a comprehensive analysis of historical donation data. Our focus involved the creation and evaluation of multiple predictive models to ascertain their performance. Subsequently, the most effective model was employed to predict and strategically target former donors, maximizing the impact of donor dollars in the upcoming marketing campaign. Notably, SAS Enterprise Miner served as the primary tool for this project."
  },
  {
    "objectID": "posts/LeBron James Career Breakdown/index.html",
    "href": "posts/LeBron James Career Breakdown/index.html",
    "title": "LeBron James Career Breakdown",
    "section": "",
    "text": "Conducts a statistical analysis of LeBron James’s performance throughout his initial 18 seasons in the NBA.\n\nIn this project, Gregory Ehlinger and I conducted a comprehensive analysis spanning the initial 18 years of LeBron James’s NBA career. Utilizing Tableau, we developed a sophisticated dashboard presenting diverse statistical facets for each year. The visualizations encompassed elements such as profile photos and jerseys, biometric data, statistical averages, shot selection, and geolocation mapping for recorded statistics."
  },
  {
    "objectID": "resume1.html",
    "href": "resume1.html",
    "title": "resume",
    "section": "",
    "text": "&lt;iframe\n  width=“1000”\n  height=“1000”\n  src=“resume.pdf”&gt;\n&lt;/iframe&gt;"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Jenetty Immaraj",
    "section": "",
    "text": "&lt;iframe\nwidth=“800”\nheight=“800”\nsrc=“resume.pdf”&gt;\n&lt;/iframe&gt;"
  },
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to a fascinating exploration of the ‘mtcars’ dataset! In this blog post, we will embark on a journey through Exploratory Data Analysis (EDA) using R. The ‘mtcars’ dataset, a classic dataset in R, contains specifications and performance measures of various car models.\n\nknitr::include_graphics(\"cars.jpg\")"
  },
  {
    "objectID": "Blog.html#step-1-loading-the-dataset",
    "href": "Blog.html#step-1-loading-the-dataset",
    "title": "Blog",
    "section": "Step 1: Loading the Dataset",
    "text": "Step 1: Loading the Dataset\nLet’s start by loading the ‘mtcars’ dataset into our R environment.\n\n# Load the mtcars dataset\ndata(mtcars)\n\nThe provided code snippet loads the ‘mtcars’ dataset, a built-in dataset available in R. This dataset contains information about various car models, including their specifications like miles per gallon (mpg), number of cylinders, horsepower, and more. By using the command data(mtcars), this code makes the ‘mtcars’ dataset available for analysis and manipulation within the R environment."
  },
  {
    "objectID": "Blog.html#unveiling-insights-a-journey-through-eda-with-the-mtcars-dataset",
    "href": "Blog.html#unveiling-insights-a-journey-through-eda-with-the-mtcars-dataset",
    "title": "Blog",
    "section": "",
    "text": "Welcome to a fascinating exploration of the ‘mtcars’ dataset! In this blog post, we will embark on a journey through Exploratory Data Analysis (EDA) using R. The ‘mtcars’ dataset, a classic dataset in R, contains specifications and performance measures of various car models.\n\nknitr::include_graphics(\"cars.jpg\")"
  },
  {
    "objectID": "Blog.html#step-2-univariate-analysis",
    "href": "Blog.html#step-2-univariate-analysis",
    "title": "Blog",
    "section": "Step 2: Univariate Analysis",
    "text": "Step 2: Univariate Analysis\n\nSummary Statistics\nTo gain an initial understanding of the dataset, let’s examine some summary statistics.\n\n# Display summary statistics\nsummary(mtcars)\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n\nThe provided code snippet showcases the functionality of R in analyzing the ‘mtcars’ dataset. With the help of the summary() function, this code efficiently presents a condensed summary of essential statistical metrics for each variable within the dataset.\nBy leveraging R’s summary() function, we gain insights into crucial statistics such as minimum and maximum values, median, quartiles, and means associated with every variable in the ‘mtcars’ dataset. Setting echo=TRUE allows the code and its resulting output to be displayed together, enabling clear visibility of both code execution and the summary statistics generated.\nThis simple yet powerful command provides a quick overview of the dataset’s key statistical measures, offering valuable insights into the range and distribution of the car attributes in the dataset.\n\n\nMileage Distribution\nNow, let’s visualize the distribution of mileage (‘mpg’) using a histogram.\n\n# Visualize distribution of mileage\nhist(mtcars$mpg, main = \"Distribution of Mileage (mpg)\", xlab = \"Miles per Gallon\")\n\n\n\n\nIn the provided code snippet, I’ve utilized R to craft a histogram that showcases the distribution pattern of mileage (measured in miles per gallon) from the ‘mtcars’ dataset. This histogram provides a visual representation of how often various mileage values occur across different car models.\nBy using the ‘hist()’ function, I’ve generated this histogram, emphasizing the x-axis as ‘Miles per Gallon’ and titling the plot as ‘Distribution of Mileage (mpg)’. This visualization effectively illustrates the spread and frequency of distinct mileage values within our dataset.\nNotably, from the histogram, we can observe a notable peak in frequency within the 15-20 miles per gallon range, indicating that a significant number of car models in the dataset fall within this mileage range."
  },
  {
    "objectID": "Blog.html#step-3-bivariate-analysis",
    "href": "Blog.html#step-3-bivariate-analysis",
    "title": "Blog",
    "section": "Step 3: Bivariate Analysis",
    "text": "Step 3: Bivariate Analysis\n\nScatter Plot: Horsepower vs. Mileage\nLet’s explore the relationship between horsepower (‘hp’) and mileage using a scatter plot.\n\n# Scatter plot for horsepower vs. mileage\nplot(mtcars$hp, mtcars$mpg, main = \"Scatter Plot: Horsepower vs. Mileage\", xlab = \"Horsepower\", ylab = \"Miles per Gallon\")\n\n\n\n\nIn this code snippet, I’ve generated a scatter plot using R. The plot visualizes the relationship between two key car attributes: horsepower and mileage (measured in miles per gallon, mpg).\nThe plot() function creates the scatter plot. I’ve specified mtcars$hp on the x-axis, representing the car’s horsepower, and mtcars$mpg on the y-axis, depicting the miles per gallon. Additionally, I’ve included a title for the plot (“Scatter Plot: Horsepower vs. Mileage”) using main, and labeled the x-axis as “Horsepower” and the y-axis as “Miles per Gallon” using xlab and ylab respectively.\nThis visualization allows us to understand the potential relationship between a car’s horsepower and its fuel efficiency, giving insight into how these factors might be correlated in the dataset.\n\n\nCorrelation Matrix\nNow, let’s quantify the relationships between mileage, horsepower, and weight using a correlation matrix.\n\n# Correlation matrix\ncor(mtcars[, c(\"mpg\", \"hp\", \"wt\")])\n\n           mpg         hp         wt\nmpg  1.0000000 -0.7761684 -0.8676594\nhp  -0.7761684  1.0000000  0.6587479\nwt  -0.8676594  0.6587479  1.0000000\n\n\nIn this code snippet using R, I’ve computed a correlation matrix for specific attributes within the ‘mtcars’ dataset. The variables ‘mpg’ (miles per gallon), ‘hp’ (horsepower), and ‘wt’ (car weight) were chosen for this analysis.\nThe output of the correlation matrix reveals the relationship between these variables. Each number in the matrix represents the correlation coefficient between two attributes. A correlation coefficient ranges from -1 to 1, where:\n\nA value of 1 indicates a perfect positive correlation.\nA value of -1 indicates a perfect negative correlation.\nA value of 0 suggests no linear relationship between the variables.\n\nFor instance, in this matrix:\n\nThe correlation between ‘mpg’ and ‘hp’ is approximately -0.776, indicating a moderate negative correlation.\nThe correlation between ‘mpg’ and ‘wt’ is around -0.868, showing a strong negative relationship.\nThe correlation between ‘hp’ and ‘wt’ is about 0.659, representing a moderate positive correlation.\n\nThis analysis helps to understand the interdependencies among these car attributes, providing insights into how changes in one attribute might affect others."
  },
  {
    "objectID": "Blog.html#step-4-multivariate-analysis",
    "href": "Blog.html#step-4-multivariate-analysis",
    "title": "Blog",
    "section": "Step 4: Multivariate Analysis",
    "text": "Step 4: Multivariate Analysis\n\nPairwise Scatterplot Matrix\nVisualize relationships involving multiple variables with a pairwise scatterplot matrix.\n\n# Pairwise scatterplot matrix\npairs(mtcars[, c(\"mpg\", \"hp\", \"wt\", \"disp\")])\n\n\n\n\nIn the provided R code snippet, I’ve generated a pairwise scatterplot matrix using the ‘pairs()’ function for selected variables from the ‘mtcars’ dataset. The variables included for analysis are ‘mpg’ (miles per gallon), ‘hp’ (horsepower), ‘wt’ (car weight), and ‘disp’ (displacement).\nThis scatterplot matrix visually presents relationships among these specific car attributes by showcasing scatterplots of each variable against every other variable in the selected set. Each scatterplot represents the correlation or pattern between two attributes, offering a comprehensive view of their interactions.\nThe resulting output is a matrix of scatterplots arranged in a grid format, where the diagonal displays histograms showcasing the distribution of individual variables. The off-diagonal elements exhibit scatterplots illustrating the relationship between pairs of attributes.\nThis visual representation aids in identifying potential correlations, trends, or patterns among the selected car attributes, providing an intuitive overview of their interrelationships within the dataset.\n\n\nBoxplot: Mileage Distribution by Number of Cylinders\nExplore how the number of cylinders (‘cyl’) influences mileage using a boxplot.\n\n# Boxplot by number of cylinders\nboxplot(mpg ~ cyl, data = mtcars, main = \"Mileage Distribution by Number of Cylinders\", xlab = \"Number of Cylinders\", ylab = \"Miles per Gallon\")\n\n\n\n\nIn this code segment using R, I’ve generated a boxplot visualization to explore the relationship between the number of cylinders in cars and their corresponding mileage (measured in miles per gallon, mpg).\nThe boxplot() function creates the graphical representation. By specifying mpg ~ cyl, I’ve directed the plot to display the distribution of mileage for different numbers of cylinders. The ‘mtcars’ dataset is utilized for this analysis.\nThe resulting boxplot visually compares the mileage distributions across various cylinder counts. The x-axis shows the number of cylinders in the cars, while the y-axis represents the miles per gallon achieved by each category. This visualization helps in understanding the variation in mileage concerning the different cylinder configurations, showcasing potential trends or differences in fuel efficiency based on the number of cylinders.\nBy observing the spread and median values depicted in the boxplot for each cylinder count category, we can gain insights into how the number of cylinders might impact the overall mileage performance of cars."
  },
  {
    "objectID": "Blog.html#conclusion",
    "href": "Blog.html#conclusion",
    "title": "Blog",
    "section": "Conclusion",
    "text": "Conclusion\nIn this journey through EDA with the ‘mtcars’ dataset, we uncovered valuable insights about the relationships between mileage, horsepower, and other specifications. Higher horsepower tends to correlate with lower mileage, and there’s significant variation in mileage based on the number of cylinders."
  },
  {
    "objectID": "Blog.html#closing-thoughts",
    "href": "Blog.html#closing-thoughts",
    "title": "Blog",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nEDA is a powerful tool for gaining a deeper understanding of your data. I encourage you to apply these techniques to your own datasets and explore the fascinating stories they hold. Happy exploring!"
  },
  {
    "objectID": "Blog.html#section",
    "href": "Blog.html#section",
    "title": "Blog",
    "section": "",
    "text": "Mileage Distribution\nNow, let’s visualize the distribution of mileage (‘mpg’) using a histogram.\n\n# Visualize distribution of mileage\nhist(mtcars$mpg, main = \"Distribution of Mileage (mpg)\", xlab = \"Miles per Gallon\")"
  },
  {
    "objectID": "docs/resume.html",
    "href": "docs/resume.html",
    "title": "Jenetty Immaraj",
    "section": "",
    "text": "&lt;iframe\nwidth=“800”\nheight=“800”\nsrc=“resume.pdf”&gt;\n&lt;/iframe&gt;"
  },
  {
    "objectID": "resume.html#veda-jenetty-immaraj",
    "href": "resume.html#veda-jenetty-immaraj",
    "title": "Resume",
    "section": "",
    "text": "vjenettyimmaraj@gmail.com"
  },
  {
    "objectID": "resume.html#professional-summary",
    "href": "resume.html#professional-summary",
    "title": "Resume",
    "section": "Professional Summary",
    "text": "Professional Summary\n\nSeasoned Data Scientist with 8+ years of experience in end-to-end data science solution development.\nSpecialized in architecting, developing, and deploying ML solutions on AWS, GCP, and Azure ML Studio.\nProficient in building machine learning and deep learning models for B2B/B2C analytics, supply chain, marketing, and finance.\nSkilled in data preprocessing, exploratory data analysis (EDA), deep learning model building, and report generation.\nExperienced with NoSQL, SQL, Shell Scripting, Automation scripts, and Tableau for data visualization.\nHolds certifications as an AWS Associate Architect and Airflow Astronomer.\nSuccessful track record includes building applications from scratch and handling multiple use cases on AWS."
  },
  {
    "objectID": "resume.html#leadership-experience",
    "href": "resume.html#leadership-experience",
    "title": "Resume",
    "section": "Leadership Experience",
    "text": "Leadership Experience\n\nShareholder's Value Creation\n\nLed the product strategy, roadmap, and implementation of analytics dashboards providing a 360-degree view of the company KPI metrics in product, growth, operations, and sales.\nExperience in driving strategic business decisions driven by quantitative data with great attention to detail.\n\n\n\nTeam Leadership\n\nDetail-oriented leader-built ramp-up roadmap for junior/senior data scientists to catalyze the onboarding time to the project.\nMentored 4 data scientists and analysts to increase productivity and unlock their potential."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\n\nBachelor of Technology in Computer Science & Engineering from JNTU-Hyderabad.\nMaster's in data science and advanced data Analytics at the University of North Texas, Denton."
  },
  {
    "objectID": "resume.html#certifications",
    "href": "resume.html#certifications",
    "title": "Resume",
    "section": "Certifications",
    "text": "Certifications\n\nData Science & Big Data Analytics - Certified by LTI of Carnegie Mellon University certification in 2016 - ranked 3rd worldwide."
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nTools\n\nPycharm, SAS, GCP cloud, AWS cloud, Microsoft Azure ML Studio, Airflow, PowerPoint, Power BI, Tableau, Excel, MS SQL Server, MySQL, PostgreSQL, Teradata, MLOps, Snowflake\n\n\n\nProgramming\n\nPython, NumPy, Pandas, Matplotlib, SciPy, Scikit-Learn, Kera's, TensorFlow, R, SQL, Spark, Hadoop, Hive\n\n\n\nAnalytics\n\nRegression, Classification, Clustering, Hypothesis Testing, A/B Testing, T-test, F-Test, ANOVA, Forecasting, statistical methods, supervised learning, unsupervised learning, cross-validation, statistical modeling, Predictive Analytics\n\n\n\nMachine Learning\n\nPrincipal Component Analysis, Natural Language Processing, Deep Learning, AI/ML algorithms, Autoencoders, Text Analytics"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Resume",
    "section": "Professional Experience",
    "text": "Professional Experience\nGENPACT - Sr Data Scientist (01-Mar-2021 - 19-Aug-2022)\nQUADRATYX - Data Scientist (25-Jan-2019 - 19-Feb-2021)\nVIRTUSA - Software Engineer (30-Sept-2015 - 24-Jan-2019)"
  },
  {
    "objectID": "resume.html#genpact---sr-data-scientist-01-mar-2021---19-aug-2022",
    "href": "resume.html#genpact---sr-data-scientist-01-mar-2021---19-aug-2022",
    "title": "Resume",
    "section": "###GENPACT - Sr Data Scientist (01-Mar-2021 - 19-Aug-2022)",
    "text": "###GENPACT - Sr Data Scientist (01-Mar-2021 - 19-Aug-2022)"
  },
  {
    "objectID": "resume.html#quadratyx--data-scientist-25-jan-2019---19-feb-2021",
    "href": "resume.html#quadratyx--data-scientist-25-jan-2019---19-feb-2021",
    "title": "Resume",
    "section": "###QUADRATYX- Data Scientist (25-jan-2019 - 19-Feb-2021)",
    "text": "###QUADRATYX- Data Scientist (25-jan-2019 - 19-Feb-2021)"
  },
  {
    "objectID": "resume.html#virtusa---software-engineer-30-sept-2015---24-jan-2019",
    "href": "resume.html#virtusa---software-engineer-30-sept-2015---24-jan-2019",
    "title": "Resume",
    "section": "###VIRTUSA - Software Engineer (30-Sept-2015 - 24-Jan-2019)",
    "text": "###VIRTUSA - Software Engineer (30-Sept-2015 - 24-Jan-2019)"
  },
  {
    "objectID": "resume.html#seasoned-data-scientist-with-8-years-of-experience-in-end-to-end-data-science-solution-development.",
    "href": "resume.html#seasoned-data-scientist-with-8-years-of-experience-in-end-to-end-data-science-solution-development.",
    "title": "Resume",
    "section": "Seasoned Data Scientist with 8+ years of experience in end-to-end data science solution development.",
    "text": "Seasoned Data Scientist with 8+ years of experience in end-to-end data science solution development."
  },
  {
    "objectID": "resume.html#specialized-in-architecting-developing-and-deploying-ml-solutions-on-aws-gcp-and-azure-ml-studio.",
    "href": "resume.html#specialized-in-architecting-developing-and-deploying-ml-solutions-on-aws-gcp-and-azure-ml-studio.",
    "title": "Resume",
    "section": "Specialized in architecting, developing, and deploying ML solutions on AWS, GCP, and Azure ML Studio.",
    "text": "Specialized in architecting, developing, and deploying ML solutions on AWS, GCP, and Azure ML Studio."
  },
  {
    "objectID": "resume.html#proficient-in-building-machine-learning-and-deep-learning-models-for-b2bb2c-analytics-supply-chain-marketing-and-finance.",
    "href": "resume.html#proficient-in-building-machine-learning-and-deep-learning-models-for-b2bb2c-analytics-supply-chain-marketing-and-finance.",
    "title": "Resume",
    "section": "Proficient in building machine learning and deep learning models for B2B/B2C analytics, supply chain, marketing, and finance.",
    "text": "Proficient in building machine learning and deep learning models for B2B/B2C analytics, supply chain, marketing, and finance."
  },
  {
    "objectID": "resume.html#skilled-in-data-preprocessing-exploratory-data-analysis-eda-deep-learning-model-building-and-report-generation.",
    "href": "resume.html#skilled-in-data-preprocessing-exploratory-data-analysis-eda-deep-learning-model-building-and-report-generation.",
    "title": "Resume",
    "section": "Skilled in data preprocessing, exploratory data analysis (EDA), deep learning model building, and report generation.",
    "text": "Skilled in data preprocessing, exploratory data analysis (EDA), deep learning model building, and report generation."
  },
  {
    "objectID": "resume.html#experienced-with-nosql-sql-shell-scripting-automation-scripts-and-tableau-for-data-visualization.",
    "href": "resume.html#experienced-with-nosql-sql-shell-scripting-automation-scripts-and-tableau-for-data-visualization.",
    "title": "Resume",
    "section": "Experienced with NoSQL, SQL, Shell Scripting, Automation scripts, and Tableau for data visualization.",
    "text": "Experienced with NoSQL, SQL, Shell Scripting, Automation scripts, and Tableau for data visualization."
  },
  {
    "objectID": "resume.html#holds-certifications-as-an-aws-associate-architect-and-airflow-astronomer.",
    "href": "resume.html#holds-certifications-as-an-aws-associate-architect-and-airflow-astronomer.",
    "title": "Resume",
    "section": "Holds certifications as an AWS Associate Architect and Airflow Astronomer.",
    "text": "Holds certifications as an AWS Associate Architect and Airflow Astronomer."
  },
  {
    "objectID": "resume.html#successful-track-record-includes-building-applications-from-scratch-and-handling-multiple-use-cases-on-aws.",
    "href": "resume.html#successful-track-record-includes-building-applications-from-scratch-and-handling-multiple-use-cases-on-aws.",
    "title": "Resume",
    "section": "Successful track record includes building applications from scratch and handling multiple use cases on AWS.",
    "text": "Successful track record includes building applications from scratch and handling multiple use cases on AWS.\n\nKey Achievements:\n\nDeveloped a churn prediction model with 82% accuracy, saving ~.9M $ annually.\nLed the conversion of a 1.2 M $ contract from a Hi-Tech customer.\nLed the development of a Proof-of-Concept project unlocking an engagement of ~300K $ within 6 months.\nProposed pricing analytics solution for an e-commerce giant with more than 50+ million users and 500k SKU which Uncovered a lift in demand 1.08X and saw an average 4% increase in revenue in A/B Tested groups."
  },
  {
    "objectID": "resume.html#professional-summary-1",
    "href": "resume.html#professional-summary-1",
    "title": "Resume",
    "section": "Professional Summary",
    "text": "Professional Summary\n\nSeasoned Data Scientist with 8+ years of experience in end-to-end data science solution development.\nSpecialized in architecting, developing, and deploying ML solutions on AWS, GCP, and Azure ML Studio.\nProficient in building machine learning and deep learning models for B2B/B2C analytics, supply chain, marketing, and finance.\nSkilled in data preprocessing, exploratory data analysis (EDA), deep learning model building, and report generation.\nExperienced with NoSQL, SQL, Shell Scripting, Automation scripts, and Tableau for data visualization.\nHolds certifications as an AWS Associate Architect and Airflow Astronomer.\nSuccessful track record includes building applications from scratch and handling multiple use cases on AWS. ### Key Achievements:\n\nDeveloped a churn prediction model with 82% accuracy, saving ~.9M $ annually.\nLed the conversion of a 1.2 M $ contract from a Hi-Tech customer.\nLed the development of a Proof-of-Concept project unlocking an engagement of ~300K $ within 6 months.\nProposed pricing analytics solution for an e-commerce giant with more than 50+ million users and 500k SKU which Uncovered a lift in demand 1.08X and saw an average 4% increase in revenue in A/B Tested groups.\n\nLeadership Experience\nShareholder's Value Creation\n\nLed the product strategy, roadmap, and implementation of analytics dashboards providing a 360-degree view of the company KPI metrics in product, growth, operations, and sales.\nExperience in driving strategic business decisions driven by quantitative data with great attention to detail.\n\nTeam Leadership\n\nDetail-oriented leader-built ramp-up roadmap for junior/senior data scientists to catalyze the onboarding time to the project.\nMentored 4 data scientists and analysts to increase productivity and unlock their potential.\n\nEducation\n\nBachelor of Technology in Computer Science & Engineering from JNTU-Hyderabad.\nMaster's in data science and advanced data Analytics at the University of North Texas, Denton.\n\nCertifications\n\nData Science & Big Data Analytics - Certified by LTI of Carnegie Mellon University certification in 2016 - ranked 3rd worldwide.\n\nTechnical Skills\nTools\n\nPycharm, Tableau, SAS, GCP cloud, AWS cloud, Microsoft Azure ML Studio, Airflow, PowerPoint, Excel, MS SQL Server, MySQL, PostgreSQL, Teradata, MLOps, Snowflake\n\nProgramming\n\nPython, NumPy, Pandas, Matplotlib, SciPy, Scikit-Learn, Kera's, TensorFlow, R, SQL, Spark, Hadoop, Hive\n\nAnalytics\n\nRegression, Classification, Clustering, Hypothesis Testing, A/B Testing, T-test, F-Test, ANOVA, Forecasting, statistical methods, supervised learning, unsupervised learning, cross-validation, statistical modeling, Predictive Analytics\n\nMachine Learning\n\nPrincipal Component Analysis, Natural Language Processing, Deep Learning, AI/ML algorithms, Autoencoders, Text Analytics\n\nOther\n\nRedshift, Hadoop, Hive, SQL, NoSQL – MongoDB, HBase"
  },
  {
    "objectID": "resume.html#key-achievements",
    "href": "resume.html#key-achievements",
    "title": "Resume",
    "section": "Key Achievements:",
    "text": "Key Achievements:\n\nDeveloped a churn prediction model with 82% accuracy, saving ~.9M $ annually.\nLed the conversion of a 1.2 M $ contract from a Hi-Tech customer.\nLed the development of a Proof-of-Concept project unlocking an engagement of ~300K $ within 6 months.\nProposed pricing analytics solution for an e-commerce giant with more than 50+ million users and 500k SKU which Uncovered a lift in demand 1.08X and saw an average 4% increase in revenue in A/B Tested groups."
  },
  {
    "objectID": "resume.html#my-best-work",
    "href": "resume.html#my-best-work",
    "title": "Resume",
    "section": "My Best Work:",
    "text": "My Best Work:\n## Projects\n### AI-Powered Production Support System for Top-Tier Banking Enterprise\n(Genpact, Client Deutsche Bank, New York, NY)\n- **Title:** AI-Powered Production Support System\n- **Goal:** Design and develop a cutting-edge Production Support System powered by Artificial Intelligence to reduce Mean Time to Recovery (MTTR).\n- **Outcome:** Achieved a 15% improvement in MTTR, 45% reduction in call center hold times, and an anticipated annual savings of $2M.\n- **Responsibilities:**\n\nDeveloped an AI Powered Production Support system using custom-tailored ML models such as Doc2Vec, SBert, and Word2Vec models for the NLP engine.\nDesigned the Technical Roadmap and Functional Architecture of the Solution.\nCollaborated with the client to understand their requirements and charted a path for project implementation.\n\n- **Tools:** GCP, NLP, Jupyter Notebooks, Pycharm\n### Football Jersey Forecasting\n(Genpact, Adidas - EU ECOM, Germany)\n- **Title:** Football Jersey Forecasting\n- **Goal:** Forecast the demand of Jerseys for- 2022 Elite 5 Clubs and 8 World Cup Federations.\n- **Outcome:** Managed a €3.2M project aimed at enhancing operational efficiency due to the transfer of star players.\n- **Responsibilities:**\n\nDefined the solution roadmap and architected and developed end-to-end solutions.\nBuilt several multivariate time series forecasting models using key features such as historic demand, team performance, Google Trends, PDP visits, etc., to predict future demand for an upcoming season.\n\n- **Tools:** Azure Cloud, Multivariate Time Series Models, MySQL, Jupyter Notebooks, Pycharm\n### Self-Learning Machines\\Auto-Learning\n(Quadratyx, Hyderabad, India)\n- **Title:** Self-Learning Machines\\Auto-Learning\n- **Goal:** Develop an AI platform that enables Sales and Product teams at Adidas to leverage Data Science as a Service for informed decision-making.\n- **Outcome:** Collaborated with fellow data scientists and engineers to build an AI platform for Click-Stream analytics to predict lead generation for Adidas’s Product teams.\n- **Responsibilities:**\n\nDeveloped and implemented an in-house custom algorithm for automated binning of data to increase accuracy and actionability of insights.\nUtilized a stack of feature selection techniques and applied machine learning models such as Logistic Regression, Decision Trees, and Random Forest with Grid Search to develop predictive models for lead generation.\n\n- **Tools:** Python, SQL, Django, Teradata\n\nAI-Powered Production Support System for Top-Tier Banking Enterprise (Genpact, Client Deutsche Bank, New York, NY)\n\nObjective: Design and develop a cutting-edge Production Support System powered by Artificial Intelligence to reduce Mean Time to Recovery (MTTR).\nOutcome: Achieved a 15% improvement in MTTR, 45% reduction in call center hold times, and an anticipated annual savings of $2M.\n\n\n\nFootball Jersey Forecasting (Genpact, Adidas - EU ECOM, Germany)\n\nObjective: Forecast the demand of Jerseys for- 2022 Elite 5 Clubs and 8 World Cup Federations.\nOutcome: Managed a €3.2M project aimed at enhancing operational efficiency due to the transfer of star players.\n\n\n\nSelf-Learning Machines\\Auto-Learning (Quadratyx, Hyderabad, India)\n\nObjective: Develop an AI platform for Click-Stream analytics to predict lead generation for Adidas's Product teams.\nOutcome: Increased accuracy and actionability of insights using in-house custom algorithm for automated binning of data.\n\n\n\nProject Evergreen Churn Analytics (Quadratyx, Hyderabad, India)\n\nObjective: Identify churn patterns in CA’s account base and develop an early warning system to reduce customer churn.\nOutcome: Deployed models providing a powerful tool to mitigate churn and improve customer retention.\n\n\n\nIntelligent Voice Assistant Solution for Seamless Reporting (Quadratyx, Hyderabad, India)\n\nObjective: Create a dynamic chatbot leveraging conversational interactions to retrieve data from SQL/NoSQL databases.\nOutcome: Designed a highly configurable product enabling querying any structured database using conversational sentences.\n\n\n\nAI-Powered Voice Analytics Agent for Microfinance Company (Quadratyx, Hyderabad, India)\n\nObjective: Minimize net default rate on payments, predict likelihood of an individual defaulting on a loan payment.\nOutcome: Developed solution to extract vital information from real-time voice recordings.\n\n\n\nProduct Tagging Classification (Quadratyx, Client: Shopify, Hyderabad, India)\n\nObjective: Improve effectiveness of Project Evergreen by assessing impact of community presence on customer churn.\nOutcome: Developed and implemented a robust text classification engine using NLP techniques and machine learning models.\n\n\n\nML WEB APP (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed an ML WebApp using Flask API backed by the Naïve Bayes model with 75% AUC for product marketing and customer success team.\n\n\n\nChatbot Development (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed a bot using RASA and chatbot LUMA framework to automate L1 & L2 service incidents, enhancing the business process by 40%.\n\n\n\nActive Learning in Churn Propensity Model (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built a churn propensity model for specific products at CA, automated feature engineering, and developed a real-time dashboard to track model and dataset drifts.\n\n\n\nStatistical Analysis (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Performed various descriptive and inferential statistics and presented insights to the team.\n\n\n\nCustomer Acquisition Modeling (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built propensity models for B2B sales, leveraging campaigns & external intent signals, resulting in an incline in lead to conversion rate by 19%."
  },
  {
    "objectID": "resume.html#key-achievements-1",
    "href": "resume.html#key-achievements-1",
    "title": "Resume",
    "section": "Key Achievements:",
    "text": "Key Achievements:\n\nAI-Powered Production Support System for Top-Tier Banking Enterprise (Genpact, Client Deutsche Bank, New York, NY)\n\nObjective: Design and develop a cutting-edge Production Support System powered by Artificial Intelligence to reduce Mean Time to Recovery (MTTR).\nOutcome: Achieved a 15% improvement in MTTR, 45% reduction in call center hold times, and an anticipated annual savings of $2M.\n\n\n\nFootball Jersey Forecasting (Genpact, Adidas - EU ECOM, Germany)\n\nObjective: Forecast the demand of Jerseys for- 2022 Elite 5 Clubs and 8 World Cup Federations.\nOutcome: Managed a €3.2M project aimed at enhancing operational efficiency due to the transfer of star players.\n\n\n\nSelf-Learning Machines\\Auto-Learning (Quadratyx, Hyderabad, India)\n\nObjective: Develop an AI platform for Click-Stream analytics to predict lead generation for Adidas's Product teams.\nOutcome: Increased accuracy and actionability of insights using in-house custom algorithm for automated binning of data.\n\n\n\nProject Evergreen Churn Analytics (Quadratyx, Hyderabad, India)\n\nObjective: Identify churn patterns in CA’s account base and develop an early warning system to reduce customer churn.\nOutcome: Deployed models providing a powerful tool to mitigate churn and improve customer retention.\n\n\n\nIntelligent Voice Assistant Solution for Seamless Reporting (Quadratyx, Hyderabad, India)\n\nObjective: Create a dynamic chatbot leveraging conversational interactions to retrieve data from SQL/NoSQL databases.\nOutcome: Designed a highly configurable product enabling querying any structured database using conversational sentences.\n\n\n\nAI-Powered Voice Analytics Agent for Microfinance Company (Quadratyx, Hyderabad, India)\n\nObjective: Minimize net default rate on payments, predict likelihood of an individual defaulting on a loan payment.\nOutcome: Developed solution to extract vital information from real-time voice recordings.\n\n\n\nProduct Tagging Classification (Quadratyx, Client: Shopify, Hyderabad, India)\n\nObjective: Improve effectiveness of Project Evergreen by assessing impact of community presence on customer churn.\nOutcome: Developed and implemented a robust text classification engine using NLP techniques and machine learning models.\n\n\n\nML WEB APP (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed an ML WebApp using Flask API backed by the Naïve Bayes model with 75% AUC for product marketing and customer success team.\n\n\n\nChatbot Development (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed a bot using RASA and chatbot LUMA framework to automate L1 & L2 service incidents, enhancing the business process by 40%.\n\n\n\nActive Learning in Churn Propensity Model (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built a churn propensity model for specific products at CA, automated feature engineering, and developed a real-time dashboard to track model and dataset drifts.\n\n\n\nStatistical Analysis (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Performed various descriptive and inferential statistics and presented insights to the team.\n\n\n\nCustomer Acquisition Modeling (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built propensity models for B2B sales, leveraging campaigns & external intent signals, resulting in an incline in lead to conversion rate by 19%."
  },
  {
    "objectID": "resume.html#project",
    "href": "resume.html#project",
    "title": "Resume",
    "section": "Project:",
    "text": "Project:\n\nAI-Powered Production Support System for Top-Tier Banking Enterprise (Genpact, Client Deutsche Bank, New York, NY)\n\n**Title:** AI-Powered Production Support System.\n**Goal:** Design and develop a cutting-edge Production Support System powered by Artificial Intelligence to reduce Mean Time to Recovery (MTTR).\n\n- **Outcome:** Achieved a 15% improvement in MTTR, 45% reduction in call center hold times, and an anticipated annual savings of $2M.\n- **Responsibilities:**\n\nDeveloped an AI Powered Production Support system using custom-tailored ML models such as Doc2Vec, SBert, and Word2Vec models for the NLP engine.\nDesigned the Technical Roadmap and Functional Architecture of the Solution.\nCollaborated with the client to understand their requirements and charted a path for project implementation.\n\n- **Tools:** GCP, NLP, Jupyter Notebooks, Pycharm\n\n\nFootball Jersey Forecasting (Genpact, Adidas - EU ECOM, Germany)\n\nObjective: Forecast the demand of Jerseys for- 2022 Elite 5 Clubs and 8 World Cup Federations.\nOutcome: Managed a €3.2M project aimed at enhancing operational efficiency due to the transfer of star players.\n\n\n\nSelf-Learning Machines\\Auto-Learning (Quadratyx, Hyderabad, India)\n\nObjective: Develop an AI platform for Click-Stream analytics to predict lead generation for Adidas's Product teams.\nOutcome: Increased accuracy and actionability of insights using in-house custom algorithm for automated binning of data.\n\n\n\nProject Evergreen Churn Analytics (Quadratyx, Hyderabad, India)\n\nObjective: Identify churn patterns in CA’s account base and develop an early warning system to reduce customer churn.\nOutcome: Deployed models providing a powerful tool to mitigate churn and improve customer retention.\n\n\n\nIntelligent Voice Assistant Solution for Seamless Reporting (Quadratyx, Hyderabad, India)\n\nObjective: Create a dynamic chatbot leveraging conversational interactions to retrieve data from SQL/NoSQL databases.\nOutcome: Designed a highly configurable product enabling querying any structured database using conversational sentences.\n\n\n\nAI-Powered Voice Analytics Agent for Microfinance Company (Quadratyx, Hyderabad, India)\n\nObjective: Minimize net default rate on payments, predict likelihood of an individual defaulting on a loan payment.\nOutcome: Developed solution to extract vital information from real-time voice recordings.\n\n\n\nProduct Tagging Classification (Quadratyx, Client: Shopify, Hyderabad, India)\n\nObjective: Improve effectiveness of Project Evergreen by assessing impact of community presence on customer churn.\nOutcome: Developed and implemented a robust text classification engine using NLP techniques and machine learning models.\n\n\n\nML WEB APP (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed an ML WebApp using Flask API backed by the Naïve Bayes model with 75% AUC for product marketing and customer success team.\n\n\n\nChatbot Development (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed a bot using RASA and chatbot LUMA framework to automate L1 & L2 service incidents, enhancing the business process by 40%.\n\n\n\nActive Learning in Churn Propensity Model (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built a churn propensity model for specific products at CA, automated feature engineering, and developed a real-time dashboard to track model and dataset drifts.\n\n\n\nStatistical Analysis (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Performed various descriptive and inferential statistics and presented insights to the team.\n\n\n\nCustomer Acquisition Modeling (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built propensity models for B2B sales, leveraging campaigns & external intent signals, resulting in an incline in lead to conversion rate by 19%."
  },
  {
    "objectID": "resume.html#projects",
    "href": "resume.html#projects",
    "title": "Resume",
    "section": "Projects:",
    "text": "Projects:\n\nAI-Powered Production Support System for Top-Tier Banking Enterprise (Genpact, Client Deutsche Bank, New York, NY)\n\nknitr::include_graphics(\"project1.png\")\n\n\n\n\n\nTitle: AI-Powered Production Support System.\nGoal: Design and develop a cutting-edge Production Support System powered by Artificial Intelligence to reduce Mean Time to Recovery (MTTR).\nOutcome: Achieved a 15% improvement in MTTR, a 45% reduction in call center hold times, and an anticipated annual savings of $2M.\nResponsibilities:\nDeveloped an AI Powered Production Support system using custom-tailored ML models such as Doc2Vec, SBert, and Word2Vec models for the NLP engine.\nDesigned the Technical Roadmap and Functional Architecture of the Solution.\nCollaborated with the client to understand their requirements and charted a path for project implementation.\nTools: GCP, NLP, Jupyter Notebooks, Pycharm.\n\n\n\nFootball Jersey Forecasting (Genpact, Adidas - EU ECOM, Germany)\n\nknitr::include_graphics(\"project2.jpg\")\n\n\n\n\n\nTitle: Football Jersey Forecasting.\nGoal: Forecast the demand for Jerseys for the Elite 5 Clubs and 8 World Cup Federations for year 2022.\nOutcome: Managed a €3.2M project aimed at enhancing operational efficiency due to the transfer of star players.\nResponsibilities:\nDefined the solution roadmap and architected and developed end-to-end solutions.\nConducted comprehensive analysis of client requirements and devised strategies for project implementation.\nBuilt several multivariate time series forecasting models using key features such as historic demand, team performance, Google Trends, PDP visits, etc., to predict future demand for an upcoming season.\nDeveloped an AutoML pipeline to facilitate easy deployment, retraining, and reuse of the model in future projects.\nTools: Azure Cloud, Multivariate Time Series Models, MySQL, Jupyter Notebooks, Pycharm.\n\n\n\nEvergreen Churn Analytics (Quadratyx, Hyderabad, India)\n\nknitr::include_graphics(\"project3.jpg\")\n\n\n\n\n\nTitle: Churn Analysis & Modeling.\nOutcome: Deployed models providing a powerful tool to mitigate churn and improve customer retention.\nResponsibilities:\nLed end-to-end delivery of the Early Warning System project, from planning to deployment, ensuring timely and high-quality delivery.\nConducted extensive data preparation from multiple sources, including the digital presence of a customer, contract history, support experience, and adoption metrics, for a data volume of 500K records.\nDeveloped a custom feature selection algorithm to reduce the number of features from 150 to 10 actionable features, improving the model’s interpretability and accuracy.\n\n- Applied various machine learning models, including Logistic Regression, Random Forest, Decision Trees, KNN- Classifier, and Naïve Bayes’, and selected Logistic Regression based on its actionability.\n\nTools: Python, SQL Server, Web Framework Django, Models Built: Logistic Regression, Decision Trees, GBM, Random Forest, Boosting.\n\n\nSelf-Learning Machines\\Auto-Learning (Quadratyx, Hyderabad, India)\n\nObjective: Develop an AI platform for Click-Stream analytics to predict lead generation for Adidas's Product teams.\nOutcome: Increased accuracy and actionability of insights using in-house custom algorithm for automated binning of data.\n\n\n\nIntelligent Voice Assistant Solution for Seamless Reporting (Quadratyx, Hyderabad, India)\n\nObjective: Create a dynamic chatbot leveraging conversational interactions to retrieve data from SQL/NoSQL databases.\nOutcome: Designed a highly configurable product enabling querying any structured database using conversational sentences.\n\n\n\nAI-Powered Voice Analytics Agent for Microfinance Company (Quadratyx, Hyderabad, India)\n\nObjective: Minimize net default rate on payments, predict likelihood of an individual defaulting on a loan payment.\nOutcome: Developed solution to extract vital information from real-time voice recordings.\n\n\n\nProduct Tagging Classification (Quadratyx, Client: Shopify, Hyderabad, India)\n\nObjective: Improve effectiveness of Project Evergreen by assessing impact of community presence on customer churn.\nOutcome: Developed and implemented a robust text classification engine using NLP techniques and machine learning models.\n\n\n\nML WEB APP (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed an ML WebApp using Flask API backed by the Naïve Bayes model with 75% AUC for product marketing and customer success team.\n\n\n\nChatbot Development (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed a bot using RASA and chatbot LUMA framework to automate L1 & L2 service incidents, enhancing the business process by 40%.\n\n\n\nActive Learning in Churn Propensity Model (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built a churn propensity model for specific products at CA, automated feature engineering, and developed a real-time dashboard to track model and dataset drifts.\n\n\n\nStatistical Analysis (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Performed various descriptive and inferential statistics and presented insights to the team.\n\n\n\nCustomer Acquisition Modeling (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built propensity models for B2B sales, leveraging campaigns & external intent signals, resulting in an incline in lead to conversion rate by 19%."
  },
  {
    "objectID": "resume.html#my-best-work-in-data-science",
    "href": "resume.html#my-best-work-in-data-science",
    "title": "Resume",
    "section": "My Best Work in Data Science:",
    "text": "My Best Work in Data Science:\n\nAI-Powered Production Support System for Top-Tier Banking Enterprise (Genpact, Client Deutsche Bank, New York, NY)\n\nknitr::include_graphics(\"project1.png\")\n\n\n\n\n\nTitle: AI-Powered Production Support System.\nGoal: Design and develop a cutting-edge Production Support System powered by Artificial Intelligence to reduce Mean Time to Recovery (MTTR).\nOutcome: Achieved a 15% improvement in MTTR, a 45% reduction in call center hold times, and an anticipated annual savings of $2M.\nResponsibilities:\nDeveloped an AI Powered Production Support system using custom-tailored ML models such as Doc2Vec, SBert, and Word2Vec models for the NLP engine.\nDesigned the Technical Roadmap and Functional Architecture of the Solution.\nCollaborated with the client to understand their requirements and charted a path for project implementation.\nTools: GCP, NLP, Jupyter Notebooks, Pycharm.\n\n\n\nFootball Jersey Forecasting (Genpact, Adidas - EU ECOM, Germany)\n\nknitr::include_graphics(\"project2.jpg\")\n\n\n\n\n\nTitle: Football Jersey Forecasting.\nGoal: Forecast the demand for Jerseys for the Elite 5 Clubs and 8 World Cup Federations for year 2022.\nOutcome: Managed a €3.2M project aimed at enhancing operational efficiency due to the transfer of star players.\nResponsibilities:\nDefined the solution roadmap and architected and developed end-to-end solutions.\nConducted comprehensive analysis of client requirements and devised strategies for project implementation.\nBuilt several multivariate time series forecasting models using key features such as historic demand, team performance, Google Trends, PDP visits, etc., to predict future demand for an upcoming season.\nDeveloped an AutoML pipeline to facilitate easy deployment, retraining, and reuse of the model in future projects.\nTools: Azure Cloud, Multivariate Time Series Models, MySQL, Jupyter Notebooks, Pycharm.\n\n\n\nEvergreen Churn Analytics (Quadratyx, Hyderabad, India)\n\nknitr::include_graphics(\"project3.jpg\")\n\n\n\n\n\nTitle: Churn Analysis & Modeling.\nOutcome: Deployed models providing a powerful tool to mitigate churn and improve customer retention.\nResponsibilities:\nLed end-to-end delivery of the Early Warning System project, from planning to deployment, ensuring timely and high-quality delivery.\nConducted extensive data preparation from multiple sources, including the digital presence of a customer, contract history, support experience, and adoption metrics, for a data volume of 500K records.\nDeveloped a custom feature selection algorithm to reduce the number of features from 150 to 10 actionable features, improving the model’s interpretability and accuracy.\n\n- Applied various machine learning models, including Logistic Regression, Random Forest, Decision Trees, KNN- Classifier, and Naïve Bayes’, and selected Logistic Regression based on its actionability.\n\nTools: Python, SQL Server, Web Framework Django, Models Built: Logistic Regression, Decision Trees, GBM, Random Forest, Boosting.\n\n\n\nOther Significant Projects\n\nSelf-Learning Machines\\Auto-Learning (Quadratyx, Hyderabad, India)\n\nObjective: Develop an AI platform for Click-Stream analytics to predict lead generation for Adidas's Product teams.\nOutcome: Increased accuracy and actionability of insights using in-house custom algorithm for automated binning of data.\n\n\n\nIntelligent Voice Assistant Solution for Seamless Reporting (Quadratyx, Hyderabad, India)\n\nObjective: Create a dynamic chatbot leveraging conversational interactions to retrieve data from SQL/NoSQL databases.\nOutcome: Designed a highly configurable product enabling querying any structured database using conversational sentences.\n\n\n\nAI-Powered Voice Analytics Agent for Microfinance Company (Quadratyx, Hyderabad, India)\n\nObjective: Minimize net default rate on payments, predict likelihood of an individual defaulting on a loan payment.\nOutcome: Developed solution to extract vital information from real-time voice recordings.\n\n\n\nProduct Tagging Classification (Quadratyx, Client: Shopify, Hyderabad, India)\n\nObjective: Improve effectiveness of Project Evergreen by assessing impact of community presence on customer churn.\nOutcome: Developed and implemented a robust text classification engine using NLP techniques and machine learning models.\n\n\n\nML WEB APP (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed an ML WebApp using Flask API backed by the Naïve Bayes model with 75% AUC for product marketing and customer success team.\n\n\n\nChatbot Development (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Developed a bot using RASA and chatbot LUMA framework to automate L1 & L2 service incidents, enhancing the business process by 40%.\n\n\n\nActive Learning in Churn Propensity Model (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built a churn propensity model for specific products at CA, automated feature engineering, and developed a real-time dashboard to track model and dataset drifts.\n\n\n\nStatistical Analysis (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Performed various descriptive and inferential statistics and presented insights to the team.\n\n\n\nCustomer Acquisition Modeling (Virtusa Pvt Ltd, Hyderabad, India)\n\nObjective: Built propensity models for B2B sales, leveraging campaigns & external intent signals, resulting in an incline in lead to conversion rate by 19%."
  }
]